{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StaffordHo/OSysmacStudio-reimagined-palm-tree/blob/main/bandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-Armed Bandit Problem**\n",
        "This classic RL problem demonstrates the\n",
        "explorationâ€“exploitation tradeoff dilemma.\n",
        "\n",
        "Imagine a gambler at a row of slot\n",
        "machines (\"one-armed bandits\") deciding\n",
        "which ones to play, how many times to\n",
        "play each one and in which order to play\n",
        "them, and whether to continue with current machine or try different one.\n",
        "\n",
        "In the problem, each slot machine provides a random reward from a probability distribution specific to that machine, that is not known a priori. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls.\n",
        "\n",
        "The crucial tradeoff the gambler faces at each trial is between \"exploitation\" of the machine that has the highest expected payoff calculated thus far and \"exploration\" to get more information about the expected payoffs of the other machines in the hope to get even better payoffs. This trade-off between exploration and exploitation is constantly faced in RL."
      ],
      "metadata": {
        "id": "LWkT9YoNlvdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "wUEC1TCpvLoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Define Constants\n",
        "#\n",
        "N_ARMS=10 #10 \"bandits\" in a row\n",
        "N_STEPS = 10000 #10K steps\n",
        "EPS = 0.1 #Epsilon"
      ],
      "metadata": {
        "id": "7bHPjuMevM2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Define the Multi-Armed Bandit class\n",
        "# (1 row of N_ARMS slot machines)\n",
        "#\n",
        "class MultiArmedBandit:\n",
        "    # Initializer\n",
        "    def __init__(self):\n",
        "       # probability distribution specific to the N_ARMS slot machine (not known before hand)\n",
        "       self.probabilities=np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
        "\n",
        "    # Simulate a single pull of slot machine arm\n",
        "    # returns a win 1 or lose 0\n",
        "    def pull(self, arm):\n",
        "        return 1 if np.random.rand() < self.probabilities[arm] else 0"
      ],
      "metadata": {
        "id": "jfoaM1IMvgYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Define the Agent using the Epsilon-Greedy strategy\n",
        "#\n",
        "class EpsilonGreedyAgent:\n",
        "    # Initializer\n",
        "    def __init__(self, epsilon):\n",
        "        self.epsilon = epsilon\n",
        "        self.est_returns = np.zeros(N_ARMS)  # Estimated expected returns for each arm\n",
        "        self.arm_counts = np.zeros(N_ARMS)  # Number of times each arm was pulled\n",
        "        self.total_rewards = np.zeros(N_ARMS) # Total rewards accumulated for each slot machine\n",
        "\n",
        "    # Select an arm to pull based on the Epsilon-Greedy strategy\n",
        "    def select_arm(self):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(N_ARMS)  # Explore: Return a random arm 0 to N_ARMS-1\n",
        "        else:\n",
        "            return np.argmax(self.est_returns)  # Exploit: Return the index of arm with best returns thus far\n",
        "\n",
        "    # Update estimated expected returns of an arm\n",
        "    # (reward is 1 or 0)\n",
        "    def update(self, arm, reward):\n",
        "        self.arm_counts[arm] += 1\n",
        "        self.total_rewards[arm] += reward\n",
        "        self.est_returns[arm] = self.total_rewards[arm] / self.arm_counts[arm]"
      ],
      "metadata": {
        "id": "Ilqfag0qwkxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Instantiate and initialize a bandit (with 10 arms) and an agent\n",
        "#\n",
        "bandits = MultiArmedBandit()\n",
        "agent = EpsilonGreedyAgent(EPS)"
      ],
      "metadata": {
        "id": "LVc2ubdizM10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Simulation for 1 Episode**\n",
        "<br>(each episode has 10,000 pulls)"
      ],
      "metadata": {
        "id": "Hv2Kq7FsIoAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Run 1 episode of the simulation with 10K pulls\n",
        "#\n",
        "for step in range(N_STEPS):\n",
        "    arm = agent.select_arm() # select arm using Epsilon-Greedy strategy\n",
        "    reward = bandits.pull(arm) # 1 or 0 (win or lose)\n",
        "    agent.update(arm, reward) # update arm's est reward"
      ],
      "metadata": {
        "id": "65XVEiXgIZvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Rewards of Each Arm = {agent.total_rewards}\")\n",
        "print(f\"Arm Counts = {agent.arm_counts}\")\n",
        "print(f\"Est Returns = {agent.est_returns}\")\n",
        "print(f\"Total rewards = {sum(agent.total_rewards)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p9RweAFJaZd",
        "outputId": "f3bc88b5-78d9-41ec-9bdc-38ac04f9166f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rewards of Each Arm = [   0.   13.   26.   27.   50.   38.   91.   75.   56. 8159.]\n",
            "Arm Counts = [ 112.  110.  102.   99.  109.   79.  130.  106.   74. 9079.]\n",
            "Est Returns = [0.         0.11818182 0.25490196 0.27272727 0.4587156  0.48101266\n",
            " 0.7        0.70754717 0.75675676 0.89866725]\n",
            "Total rewards = 8535.0\n"
          ]
        }
      ]
    }
  ]
}